{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "Created on Tue Oct 29 15:26:06 2013\n",
      "\n",
      "@author: ruifang\n",
      "\"\"\"\n",
      "\n",
      "#################DECLARATION##############\n",
      "import nipype.interfaces.spm as spm         # the spm interfaces\n",
      "import nipype.interfaces.spm.utils as spmu\n",
      "import nipype.interfaces.fsl as fsl          # fsl\n",
      "import nipype.pipeline.engine as pe         # the workflow and node wrappers\n",
      "import nipype.interfaces.matlab as mlab\n",
      "mlab.MatlabCommand.set_default_paths('/SCR/material/spm8/spm8')\n",
      "\n",
      "# Tell fsl to generate all output in uncompressed nifti format\n",
      "fsl.FSLCommand.set_default_output_type('NIFTI')\n",
      "import os\n",
      "import re\n",
      "import glob    \n",
      "from nipype.interfaces.io import DataSink\n",
      "import os.path as op\n",
      "\n",
      "from nipype.algorithms.misc import TSNR      #Computes the time-course SNR for a time series\n",
      "import nipype.interfaces.utility as util\n",
      "##########################################"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/raid1/margulies/.local/lib/python2.7/site-packages/scipy/__init__.py:111: UserWarning: Numpy 1.6.2 or above is recommended for this version of scipy (detected version 1.6.1)\n",
        "  UserWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subjects = [\"M5\",\"M12\"]\n",
      "fun_file = dict()\n",
      "anat_file = dict()\n",
      "\n",
      "\n",
      "for name in subjects:\n",
      "    str = '/SCR/datasets-for-tracer-validation/tracer-validation/'+name+'/reoriented/'\n",
      "    input_file = []\n",
      "    for fname in os.listdir(str):\n",
      "        if os.path.isdir(str+fname):         \n",
      "            m = re.match(\"^S[0-9]*\", fname)\n",
      "            if m:        \n",
      "                os.chdir(str+fname)\n",
      "                for files in glob.glob(\"*.nii\"):\n",
      "                    input_file.insert(1,str+fname+\"/\"+files)  \n",
      "    input_file.sort()\n",
      "    fun_file[name] = input_file\n",
      "    \n",
      "    input_file2 = []\n",
      "    os.chdir(str+\"anat\")\n",
      "    for files in glob.glob(\"*.nii\"):\n",
      "        input_file2.insert(1,str+\"anat/\"+files)\n",
      "    anat_file[name] = input_file2\n",
      "\n",
      "\n",
      "def extract_noise_components(realigned_file, noise_mask_file, num_components,motion_file):\n",
      "    \"\"\"Derive components most reflective of physiological noise\n",
      "    \"\"\"\n",
      "    import os\n",
      "    from nibabel import load\n",
      "    import numpy as np\n",
      "    import scipy as sp\n",
      "    from scipy.signal import detrend\n",
      "    imgseries = load(realigned_file)\n",
      "    noise_mask = load(noise_mask_file)\n",
      "    voxel_timecourses = imgseries.get_data()[np.nonzero(noise_mask.get_data())]\n",
      "    for timecourse in voxel_timecourses:\n",
      "        timecourse[:] = detrend(timecourse, type='constant')\n",
      "    u,s,v = sp.linalg.svd(voxel_timecourses, full_matrices=False)\n",
      "    components_file = os.path.join(os.getcwd(), 'noise_components.txt')\n",
      "    np.savetxt(components_file, v[:num_components, :].T)\n",
      "    \n",
      "    mc_file = open(motion_file, 'r')\n",
      "    lines = []\n",
      "    for line in mc_file:\n",
      "        lines.append(line.strip())\n",
      "    noise_file = open(components_file,'r')\n",
      "    count = 0\n",
      "    \n",
      "    for line in noise_file:\n",
      "        lines[count] = line.strip()+' '+lines[count]\n",
      "        count = count+1\n",
      "    \n",
      "    noise_file = open(components_file,'w+')\n",
      "    for line in lines:\n",
      "        noise_file.write(line+\"\\n\")\n",
      "        \n",
      "    return components_file\n",
      "\n",
      "\n",
      "def session_cmb(ts_norm_list,full_cor):\n",
      "    import csv    \n",
      "    import numpy as np\n",
      "    import collections\n",
      "    from MaxCorrelationTransformer import partial_correlation\n",
      "    print partial_correlation # no need for pythonFunction.\n",
      "    from scipy.stats import pearsonr\n",
      "    import os\n",
      "    \n",
      "    count = 0\n",
      "    onefile = dict() \n",
      "    sort_onefile = dict()\n",
      "    for sfile in ts_norm_list:\n",
      "        file = open(sfile, 'rb')\n",
      "        data = csv.reader(file, delimiter='\\t')\n",
      "        table = [row for row in data]\n",
      "        if count ==0:\n",
      "            for line in table:\n",
      "                ls = line[1:len(line)]\n",
      "                ls_new = [x for x in ls if str(x) != '']\n",
      "                arr = np.array(ls_new,dtype='float')\n",
      "                onefile[int(line[0])] = arr\n",
      "            count+=1\n",
      "            sort_onefile = collections.OrderedDict(sorted(onefile.items()))  \n",
      "        else:\n",
      "            for line in table:\n",
      "                ls = line[1:len(line)]\n",
      "                ls_new = [x for x in ls if str(x) != '']\n",
      "                arr = np.array(ls_new,dtype='float')\n",
      "                cur_arr = sort_onefile.get(int(line[0]))\n",
      "                sort_onefile[int(line[0])] = np.concatenate((cur_arr, arr), axis=0)\n",
      "    \n",
      "    sort_onefile = collections.OrderedDict(sorted(sort_onefile.items())) \n",
      "    \n",
      "    cmb_file = os.path.join(os.getcwd(), 'cmb_mean_ts.txt')\n",
      "    with open(cmb_file, 'w') as f:\n",
      "        for key in sort_onefile:\n",
      "            f.write(\"%s\\t\" % key)\n",
      "            for i in sort_onefile[key]:\n",
      "                f.write(\"%f\\t\" %i)\n",
      "            f.write(\"\\n\") \n",
      "                       \n",
      "    cor_mat = []\n",
      "    for line in sort_onefile.iterkeys():\n",
      "        cor_mat.append(sort_onefile[line])\n",
      "        \n",
      "    if full_cor:\n",
      "        out_full_cor_mat = os.path.join(os.getcwd(),'full_cor_mat.txt')\n",
      "        f_full_mat = open(out_full_cor_mat,'wt')\n",
      "        out_full_cor_pval = os.path.join(os.getcwd(),'full_cor_pval.txt')\n",
      "        full_pval = open(out_full_cor_pval,'wt')\n",
      "    else:\n",
      "        out_full_cor_mat = os.path.join(os.getcwd(),'partial_cor_mat.txt')\n",
      "        f_full_mat = open(out_full_cor_mat,'wt')\n",
      "        out_full_cor_pval = os.path.join(os.getcwd(),'partial_cor_pval.txt')\n",
      "        full_pval = open(out_full_cor_pval,'wt')\n",
      "    \n",
      "    if full_cor:      \n",
      "            x = np.array(cor_mat,np.float32)\n",
      "            t = len(x)\n",
      "            temp = range(0, t, 1)\n",
      "\n",
      "            for i in temp:\n",
      "                for j in temp:\n",
      "                    val1 = np.array(cor_mat[i],np.float32,ndmin = 1)\n",
      "                    val2 = np.array(cor_mat[j],np.float32,ndmin = 1)\n",
      "                    (ret,pval) = pearsonr(val1,val2)\n",
      "                    f_full_mat.write('%f' %ret + '\\t') \n",
      "                    full_pval.write('%f' %pval + '\\t')\n",
      "                f_full_mat.write('\\n')  \n",
      "                full_pval.write('\\n')\n",
      "    else:   \n",
      "            x = np.array(cor_mat,np.float32)\n",
      "            t = len(x)\n",
      "            temp = range(0, t, 1)\n",
      "            \n",
      "            for i in temp:\n",
      "                for j in temp:\n",
      "                    if i==j:\n",
      "                        f_full_mat.write('1'+'\\t')\n",
      "                        full_pval.write('1'+'\\t')\n",
      "                    else:\n",
      "                        new_list = []\n",
      "                        for k in temp:\n",
      "                            if k!=i and k!=j:\n",
      "                                ret = cor_mat[k]\n",
      "                                new_list.append(ret) \n",
      "                        z = np.array(new_list,np.float32) \n",
      "                        val1 = np.array(cor_mat[i],np.float32,ndmin = 1)\n",
      "                        val2 = np.array(cor_mat[j],np.float32,ndmin = 1)\n",
      "                        (ret,pval) = partial_correlation(val1,val2,np.transpose(z))\n",
      "                        f_full_mat.write('%f' %ret + '\\t') \n",
      "                        full_pval.write('%f' %pval + '\\t')\n",
      "                f_full_mat.write('\\n')  \n",
      "                full_pval.write('\\n')        \n",
      "    return (cmb_file,out_full_cor_mat,out_full_cor_pval)\n",
      "\n",
      "\n",
      "def session_norm(ts_mean_file):\n",
      "    import csv\n",
      "    import numpy as np\n",
      "    import collections\n",
      "    import os\n",
      "    file = open(ts_mean_file, 'rb')\n",
      "    data = csv.reader(file, delimiter='\\t')\n",
      "    table = [row for row in data]\n",
      "    \n",
      "    norm_file = dict()    \n",
      "    \n",
      "    for line in table:\n",
      "        ls = line[1:len(line)]\n",
      "        ls_new = [x for x in ls if str(x) != '']\n",
      "        arr = np.array(ls_new,dtype='float')\n",
      "        norm_arr = [(x-arr.mean())/arr.var() for x in arr]\n",
      "        norm_file[int(line[0])] = norm_arr\n",
      "\n",
      "    sort_norm_file = collections.OrderedDict(sorted(norm_file.items()))    \n",
      "    norm_mean_ts = os.path.join(os.getcwd(), 'norm_mean_ts.txt')\n",
      "    with open(norm_mean_ts, 'w') as f:\n",
      "        for key in sort_norm_file:\n",
      "            f.write(\"%s\\t\" % key)\n",
      "            for i in sort_norm_file[key]:\n",
      "                f.write(\"%f\\t\" %i)\n",
      "            f.write(\"\\n\")         \n",
      "    return norm_mean_ts\n",
      "\n",
      "\n",
      "def extract_subrois(timeseries_file, label_file,full_cor):\n",
      "    \"\"\"Extract voxel time courses for each subcortical roi index\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    timeseries_file: a 4D Nifti file\n",
      "    label_file: a 3D file containing rois in the same space/size of the 4D file\n",
      "\n",
      "    \"\"\"\n",
      "    from MaxCorrelationTransformer import partial_correlation\n",
      "    print partial_correlation # no need for pythonFunction.\n",
      "    import nibabel as nb\n",
      "    import numpy as np\n",
      "    import os \n",
      "    from scipy.stats import pearsonr\n",
      "    \n",
      "    img = nb.load(timeseries_file)\n",
      "    data = img.get_data()    \n",
      "       \n",
      "    roiimg = nb.load(label_file)\n",
      "    rois = roiimg.get_data()\n",
      "    \n",
      "    series_header = img.get_header()\n",
      "    vol_num = series_header.get_data_shape()[3]\n",
      "    \n",
      "    Rois = np.unique(rois)\n",
      "    region_list = list(Rois)\n",
      "    region_list.sort()\n",
      "    region_list.remove(0)\n",
      "    region_list = [x for x in region_list if str(x) != 'nan']\n",
      "    region_list = map(int, region_list) \n",
      "    cor_mat = []\n",
      "    \n",
      "      \n",
      "    out_ts_file = os.path.join(os.getcwd(), 'roi_timeseries.txt')\n",
      "    out_ts_mean_file = os.path.join(os.getcwd(),'roi_timeseries_mean.txt')\n",
      "    f = open(out_ts_mean_file,'wt')\n",
      "    \n",
      "    if full_cor:\n",
      "        out_full_cor_mat = os.path.join(os.getcwd(),'full_cor_mat.txt')\n",
      "        f_full_mat = open(out_full_cor_mat,'wt')\n",
      "        out_full_cor_pval = os.path.join(os.getcwd(),'full_cor_pval.txt')\n",
      "        full_pval = open(out_full_cor_pval,'wt')\n",
      "    else:\n",
      "        out_full_cor_mat = os.path.join(os.getcwd(),'partial_cor_mat.txt')\n",
      "        f_full_mat = open(out_full_cor_mat,'wt')\n",
      "        out_full_cor_pval = os.path.join(os.getcwd(),'partial_cor_pval.txt')\n",
      "        full_pval = open(out_full_cor_pval,'wt')\n",
      "        \n",
      "    with open(out_ts_file, 'wt') as fp:\n",
      "        for fsindex in region_list:\n",
      "            ijk = np.nonzero(rois == fsindex)\n",
      "            ts = data[ijk]\n",
      "            arr_sum = [0]*vol_num\n",
      "            for i0, row in enumerate(ts):\n",
      "                if type(row).__name__=='float':\n",
      "                    fp.write('%d,%d,%d,%d' % (fsindex, ijk[0][i0], ijk[1][i0], ijk[2][i0]) + '\\n')\n",
      "                else:    \n",
      "                    fp.write('%d,%d,%d,%d,' % (fsindex, ijk[0][i0], ijk[1][i0], ijk[2][i0]) + ','.join(['%.10f' % val for val in row]) + '\\n')                  \n",
      "                    count = 0\n",
      "                    for val in row:\n",
      "                        arr_sum[count] = arr_sum[count] + val\n",
      "                        count+=1\n",
      "            final = [0]*vol_num\n",
      "            i = 0\n",
      "            f.write('%d' %fsindex + '\\t')\n",
      "            for val in arr_sum:\n",
      "                final[i] = val/len(ts)\n",
      "                f.write('%f' %final[i] + '\\t')\n",
      "                i+=1\n",
      "            f.write('\\n')\n",
      "            cor_mat.append(final)     \n",
      "        #calculate correlation matrix\n",
      "        if full_cor:      \n",
      "            x = np.array(cor_mat,np.float32)\n",
      "            t = len(x)\n",
      "            temp = range(0, t, 1)\n",
      "\n",
      "            for i in temp:\n",
      "                for j in temp:\n",
      "                    val1 = np.array(cor_mat[i],np.float32,ndmin = 1)\n",
      "                    val2 = np.array(cor_mat[j],np.float32,ndmin = 1)\n",
      "                    (ret,pval) = pearsonr(val1,val2)\n",
      "                    f_full_mat.write('%f' %ret + '\\t') \n",
      "                    full_pval.write('%f' %pval + '\\t')\n",
      "                f_full_mat.write('\\n')  \n",
      "                full_pval.write('\\n')\n",
      "        else:   \n",
      "            x = np.array(cor_mat,np.float32)\n",
      "            t = len(x)\n",
      "            temp = range(0, t, 1)\n",
      "            \n",
      "            for i in temp:\n",
      "                for j in temp:\n",
      "                    if i==j:\n",
      "                        f_full_mat.write('1'+'\\t')\n",
      "                        full_pval.write('1'+'\\t')\n",
      "                    else:\n",
      "                        new_list = []\n",
      "                        for k in temp:\n",
      "                            if k!=i and k!=j:\n",
      "                                ret = cor_mat[k]\n",
      "                                new_list.append(ret) \n",
      "                        z = np.array(new_list,np.float32) \n",
      "                        val1 = np.array(cor_mat[i],np.float32,ndmin = 1)\n",
      "                        val2 = np.array(cor_mat[j],np.float32,ndmin = 1)\n",
      "                        (ret,pval) = partial_correlation(val1,val2,np.transpose(z))\n",
      "                        f_full_mat.write('%f' %ret + '\\t') \n",
      "                        full_pval.write('%f' %pval + '\\t')\n",
      "                f_full_mat.write('\\n')  \n",
      "                full_pval.write('\\n')\n",
      "              \n",
      "    return (out_ts_file,out_ts_mean_file,out_full_cor_mat,out_full_cor_pval)\n",
      "\n",
      "\n",
      "def pval_correction(full_pval):\n",
      "    from rpy2.robjects.packages import importr\n",
      "    from rpy2.robjects.vectors import FloatVector\n",
      "    import csv\n",
      "    import os \n",
      "    \n",
      "    stats = importr('stats')    \n",
      "    file = open(full_pval, 'rb')\n",
      "    data = csv.reader(file, delimiter='\\t')\n",
      "    table = [filter(None, row) for row in data]\n",
      "        \n",
      "    pval = []\n",
      "    for line in table:\n",
      "        for cell in line:\n",
      "            pval.append(float(cell))\n",
      "            \n",
      "    pval_adjust = stats.p_adjust(FloatVector(pval), method = 'BH')\n",
      "    \n",
      "    col = len(table)\n",
      "       \n",
      "    count = 0\n",
      "    for v in pval_adjust:\n",
      "        r = int(count/col)\n",
      "        c = int(count%col)\n",
      "        table[r][c] = v\n",
      "        count = count+1\n",
      "        \n",
      "    ls = full_pval.strip().split('/')    \n",
      "    name = ls[len(ls)-1].strip().split('.')\n",
      "    new_name = str(name[0])+\"_adjust.txt\"\n",
      "    out_full_cor_pval = os.path.join(os.getcwd(),new_name)\n",
      "           \n",
      "    with open(out_full_cor_pval,'wt') as f:\n",
      "        for line in table:\n",
      "            for cell in line:\n",
      "                f.write(str(cell) + \"\\t\")\n",
      "            f.write(\"\\n\")    \n",
      "    \n",
      "    return out_full_cor_pval\n",
      "\n",
      "\n",
      "def ResultPlot(fun_cor_mat,full_pval):\n",
      "    import xlrd\n",
      "    \n",
      "    nii_acr = dict()\n",
      "    acr_nii = dict()\n",
      "    \n",
      "    workbook = xlrd.open_workbook('/SCR/datasets-for-tracer-validation/Project/Markov_Parcellation_niftii_codes.xls')\n",
      "    worksheet = workbook.sheet_by_name('Sheet1')\n",
      "    num_rows = worksheet.nrows - 1\n",
      "    curr_row = -1\n",
      "    while curr_row < num_rows:\n",
      "        if curr_row == -1:\n",
      "            curr_row += 1\n",
      "            continue\n",
      "        curr_row += 1\n",
      "        row = worksheet.row(curr_row)\n",
      "        \n",
      "        if type(row[1].value).__name__=='float':\n",
      "            row[1].value=int(row[1].value)\n",
      "        else:\n",
      "            row[1].value = str(row[1].value).lower()\n",
      "        nii_acr[int(row[0].value)] = str(row[1].value)\n",
      "        acr_nii[str(row[1].value)] = int(row[0].value)\n",
      "    \n",
      "    row_idx = dict()\n",
      "    col_idx = dict()\n",
      "    true_val = dict()\n",
      "    true_val_binary = dict()\n",
      "    \n",
      "    table = []\n",
      "    binary_table = []\n",
      "    \n",
      "    #file = open('/SCR/datasets-for-tracer-validation/Project/Connectivity.Macaque.DBV.23.45.txt', 'rb')\n",
      "    file = open('/SCR/datasets-for-tracer-validation/Project/Connectivity.Macaque.DBV.23.45.max.txt', 'rb')\n",
      "    #file = open('/SCR/datasets-for-tracer-validation/Project/Connectivity.Macaque.DBV.23.45.avg.txt', 'rb')\n",
      "    row_count = 0\n",
      "    for row in file:\n",
      "        arr = row.strip().split('\\t')\n",
      "        line = []\n",
      "        binary_line = []\n",
      "        if row_count ==0:\n",
      "            row_count += 1\n",
      "            idx = 0;\n",
      "            for val in arr:\n",
      "                col_idx[idx] = str(val)\n",
      "                idx += 1\n",
      "        else:  \n",
      "            arr_count = 0\n",
      "            for i in arr:\n",
      "                if arr_count ==0:\n",
      "                    row_idx[str(arr[0])] = int(row_count-1)\n",
      "                    arr_count +=1\n",
      "                else:\n",
      "                    line.append(float(i))    \n",
      "                    true_val[str(acr_nii.get(str(arr[0]).lower()))+\"_\"+str(acr_nii.get(str(col_idx[arr_count-1]).lower()))] = float(i)\n",
      "                    if(float(i)>0):\n",
      "                        binary_line.append(1)\n",
      "                        true_val_binary[str(acr_nii.get(str(arr[0]).lower()))+\"_\"+str(acr_nii.get(str(col_idx[arr_count-1]).lower()))] = 1\n",
      "                    else:\n",
      "                        binary_line.append(0)\n",
      "                        true_val_binary[str(acr_nii.get(str(arr[0]).lower()))+\"_\"+str(acr_nii.get(str(col_idx[arr_count-1]).lower()))] = 0\n",
      "                    arr_count += 1\n",
      "                    \n",
      "            row_count += 1\n",
      "            table.append(line)\n",
      "            binary_table.append(binary_line)    \n",
      "    \n",
      "    sort_true_val = sorted(true_val.iteritems(), key=lambda (k,v): (v,k),reverse=True)\n",
      "    \n",
      "    thresh = len(sort_true_val)*0.01\n",
      "    count = 0\n",
      "    for k in sort_true_val:\n",
      "        k_name = str(k[0])\n",
      "        k_val = float(k[1])\n",
      "        if(count<thresh) and k_val>0:\n",
      "            true_val_binary[k_name] = 1\n",
      "        else:\n",
      "            true_val_binary[k_name] = 0\n",
      "        count = count+1\n",
      "   \n",
      "    \n",
      "    print __doc__\n",
      "    \n",
      "    import pylab as pl\n",
      "    import nibabel as nb\n",
      "    import numpy as np\n",
      "    from sklearn.metrics import roc_curve, auc\n",
      "    import os\n",
      "    from math import log\n",
      "    from matplotlib.gridspec import GridSpec\n",
      "    ###############\n",
      "    roiimg = nb.load('/SCR/datasets-for-tracer-validation/Project/Markov_Parcellation_3D.nii')\n",
      "    rois = roiimg.get_data()  \n",
      "    Rois = np.unique(rois)\n",
      "    region_list = list(Rois)\n",
      "    region_list.sort()\n",
      "    region_list.remove(0)\n",
      "    region_list = map(int, region_list) \n",
      "\n",
      "    region_map = dict()\n",
      "    count = 0\n",
      "    for i in region_list:\n",
      "        region_map[count] = i\n",
      "        count += 1         \n",
      "    \n",
      "    f = open(fun_cor_mat,'r')\n",
      "    f_pval = open(full_pval,'r')\n",
      "    \n",
      "    fun_cor_map = dict()\n",
      "    fun_cor_map_real = dict()\n",
      "    fun_pval_map_real = dict()\n",
      "    \n",
      "    pval = []\n",
      "    for line in f_pval:\n",
      "        arr = line.strip().split('\\t')\n",
      "        pval.append(arr)\n",
      "    pval_mat = np.array(pval,np.float32)    \n",
      "               \n",
      "    row = 0\n",
      "    for line in f:\n",
      "        arr = line.strip().split('\\t')\n",
      "        col = 0\n",
      "        for cell in arr:\n",
      "            t = 0\n",
      "            p_val = pval_mat[row,col]\n",
      "            if p_val<=0.05 and float(cell)>0:\n",
      "                t = 1\n",
      "            fun_cor_map[str(region_map.get(row))+'_'+str(region_map.get(col))] = t\n",
      "            fun_cor_map_real[str(region_map.get(row))+'_'+str(region_map.get(col))] = float(cell)\n",
      "            fun_pval_map_real[str(region_map.get(row))+'_'+str(region_map.get(col))] = p_val\n",
      "            col += 1 \n",
      "        row += 1\n",
      "\n",
      "    ##############    \n",
      "    real = true_val_binary.values()\n",
      "    \n",
      "    test = [0]*len(real)\n",
      "    count = 0\n",
      "    real = [0]*len(real)\n",
      "    for k in true_val_binary.keys():\n",
      "         test[count] = fun_cor_map_real.get(k)\n",
      "         real[count] = true_val_binary.get(k)\n",
      "         count += 1     \n",
      "        \n",
      "    fpr, tpr, thresholds = roc_curve(real, test, pos_label=1)\n",
      "    \n",
      "    real_scatter1 = true_val.values()\n",
      "    count = 0\n",
      "    real_scatter = [0]*len(real_scatter1)\n",
      "    for val in real_scatter1:\n",
      "        real_scatter[count] = log(float(val+1))   #ln\n",
      "        count = count+1\n",
      "   \n",
      "    test_scatter = [0]*len(real_scatter)\n",
      "    count = 0\n",
      "    for k in true_val.keys():\n",
      "        test_scatter[count] = float(fun_cor_map_real.get(k))\n",
      "        count += 1\n",
      "    \n",
      "    roc_auc = auc(fpr, tpr)\n",
      "    print \"Area under the ROC curve : %f\" % roc_auc\n",
      "\n",
      "    f = pl.figure(figsize=(24, 12), dpi=80)   \n",
      "    #ROC\n",
      "    gs1 = GridSpec(3, 3)\n",
      "    gs1.update(left=0.05, right=0.48, wspace=0.03)\n",
      "    ax1 = pl.subplot(gs1[:, :])  \n",
      "    # Plot ROC curve\n",
      "    ax1.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
      "    ax1.plot([0, 1], [0, 1], 'k--')\n",
      "    ax1.set_xlim([0.0, 1.0])\n",
      "    ax1.set_ylim([0.0, 1.0])\n",
      "    ax1.set_xlabel('False Positive Rate')\n",
      "    ax1.set_ylabel('True Positive Rate')\n",
      "    ax1.set_title('Receiver operating characteristic')\n",
      "    ax1.legend(loc=\"lower right\")\n",
      "\n",
      "    x = real_scatter\n",
      "    y = test_scatter\n",
      "    \n",
      "    # Calculate number of bins based on binsize for both x and y\n",
      "    min_x_data, max_x_data = np.min(x), np.max(x)\n",
      "    #binsize = 0.2\n",
      "    #num_x_bins = np.floor((max_x_data - min_x_data) / binsize)\n",
      "    num_x_bins = 50\n",
      "    \n",
      "    min_y_data, max_y_data = np.min(y), np.max(y)\n",
      "    #binsize = 0.1\n",
      "    #num_y_bins = np.floor((max_y_data - min_y_data) / binsize)\n",
      "    num_y_bins = 50\n",
      "    \n",
      "    nullfmt = pl.NullFormatter()\n",
      "    left, width = 0.1, 0.4\n",
      "    bottom, height = 0.1, 0.4\n",
      "    #bottom_h = left_h = left + width + 0.02\n",
      "    \n",
      "    gs2 = GridSpec(3, 3)\n",
      "    gs2.update(left=0.53, right=0.95, hspace=0.16,wspace=0.08)\n",
      "    axScatter = pl.subplot(gs2[:-1, :-1])\n",
      "    axScatter.set_xlabel('ln(1+A_strength)')\n",
      "    axScatter.set_ylabel('F_correlation')\n",
      "    axScatter.set_title('Anatomical strength vs. Functional correlation')\n",
      "    axScatter.set_xlim(0,1)\n",
      "    axScatter.set_ylim(-1,1)\n",
      "    \n",
      "    #rotate histogram of y-axis of scatter plot\n",
      "    axHistY = pl.subplot(gs2[:-1, -1])\n",
      "    axHistY.set_ylim(-1, 1)\n",
      "    \n",
      "    #histogram of x-axis of scatter plot\n",
      "    axHistX = pl.subplot(gs2[-1, :-1])\n",
      "    axHistX.set_xlim(0, 1)\n",
      " \n",
      "    # Remove labels from histogram edges touching scatter plot\n",
      "    axHistX.xaxis.set_major_formatter(nullfmt)\n",
      "    axHistY.yaxis.set_major_formatter(nullfmt)\n",
      "    \n",
      "    # Draw scatter plot\n",
      "    axScatter.scatter(x, y, marker='o', color = 'darkblue', edgecolor='none', s=50, alpha=0.5)\n",
      "    \n",
      "    # Draw x-axis histogram\n",
      "    axHistX.hist(x, num_x_bins, ec='red',fc='green', histtype='bar')\n",
      "    \n",
      "    # Draw y-axis histogram\n",
      "    axHistY.hist(y, num_y_bins, ec='red', fc='red', histtype='step', orientation='horizontal')\n",
      "    \n",
      "    out_image = os.path.join(os.getcwd(),'result_plot.png')\n",
      "    pl.savefig(out_image)\n",
      "    #pl.show()    \n",
      "    return out_image\n",
      "\n",
      "\n",
      "def Benchmark_Calculation(subject,glm,full_cor,combined):\n",
      "\n",
      "    sinker = pe.Node(DataSink(), name='sinker') \n",
      "    \n",
      "    #remove the first several volumes\n",
      "    \n",
      "    remove_vol = pe.MapNode(fsl.ExtractROI(t_min=3, t_size=-1),iterfield=['in_file'],name=\"remove_volumes\")\n",
      "    remove_vol.inputs.in_file = fun_file[subject]   \n",
      "    \n",
      "    \"\"\"\n",
      "    Preprocessing-realignment: The first image of each session is aligned to the first image of the first session, and then to the mean of each session\n",
      "    \"\"\"\n",
      "    realigner = pe.Node(interface = spm.Realign(),name = 'realign')\n",
      "    #realigner.inputs.in_files = fun_file[subject]\n",
      "    realigner.inputs.register_to_mean = True\n",
      "    realigner.inputs.jobtype = 'estwrite'\n",
      "    realigner.inputs.write_which = [2,1]\n",
      "    \n",
      "    \"\"\"\n",
      "    Preprocessing-coregistration: ref = mean functional from realignment & source = anatomical image\n",
      "    \"\"\"\n",
      "    coreg = pe.Node(interface = spm.Coregister(),name = 'coregister')\n",
      "    #coreg.inputs.target = realigner.outputs.mean_image    #mean functional image\n",
      "    coreg.inputs.source = anat_file[subject]    #anatomical image\n",
      "    coreg.inputs.jobtype = 'estimate'     #without resclice\n",
      "    \n",
      "    \"\"\"\n",
      "    Preprocessing-slice timing\n",
      "    \"\"\"    \n",
      "    slicetime = pe.Node(interface = spm.SliceTiming(), name = \"SliceTiming\")\n",
      "    slicetime.inputs.num_slices = 30\n",
      "    slicetime.inputs.time_repetition = 2\n",
      "    slicetime.inputs.time_acquisition = slicetime.inputs.time_repetition - (slicetime.inputs.time_repetition/slicetime.inputs.num_slices)\n",
      "    slicetime.inputs.slice_order = range(1,31,1)\n",
      "    slicetime.inputs.ref_slice = 1    \n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    Noise ROI from the highest TSNR\n",
      "    \"\"\"    \n",
      "    tsnr = pe.MapNode(TSNR(regress_poly=2),iterfield=['in_file'],name='tsnr')\n",
      "    getthresh = pe.MapNode(interface=fsl.ImageStats(op_string='-p 98'),iterfield=['in_file'],\n",
      "                           name='getthreshold')\n",
      "    threshold_stddev = pe.MapNode(fsl.Threshold(), iterfield=['in_file','thresh'],name='threshold')    \n",
      "    \n",
      "    \"\"\"\n",
      "    noise mask with white matter & csf\n",
      "    \"\"\"\n",
      "    threshCSFseg = pe.Node(interface = fsl.ImageMaths(op_string = ' -thr .80 -uthr 1 -bin '),\n",
      "                       name = 'threshcsfsegmask')\n",
      "                       \n",
      "    threshWMseg = pe.Node(interface = fsl.ImageMaths(op_string = ' -thr .80 -uthr 1 -bin '),\n",
      "                       name = 'threshwmsegmask')                   \n",
      "    \n",
      "    threshbothseg = pe.Node(interface = fsl.BinaryMaths(),name = \"theshwmcsfmask\")\n",
      "    threshbothseg.inputs.operation = 'add'\n",
      "    threshbothseg.inputs.terminal_output = 'file'                 \n",
      "    \n",
      "    reslicemask = pe.Node(interface = spmu.ResliceToReference(), name = \"ReslicedMask\")\n",
      "    reslicemask.inputs.interpolation = 0    \n",
      "    \n",
      "    \"\"\"\n",
      "    Compcor algorithms\n",
      "    \"\"\"    \n",
      "    compcor = pe.MapNode(util.Function(input_names=['realigned_file',\n",
      "                                                 'noise_mask_file',\n",
      "                                                 'num_components',\n",
      "                                                 'motion_file'],\n",
      "                                     output_names=['noise_components'],\n",
      "                                     function=extract_noise_components),iterfield = ['realigned_file','motion_file'],\n",
      "                       name='compcorr')\n",
      "    compcor.inputs.num_components = 6\n",
      "    \n",
      "    \"\"\"\n",
      "    bandpass filter\n",
      "    \"\"\"    \n",
      "    remove_noise = pe.MapNode(fsl.FilterRegressor(filter_all=True),iterfield = ['in_file','design_file'],name='remove_noise')\n",
      "    bandpass_filter = pe.MapNode(fsl.TemporalFilter(),iterfield = ['in_file'],name='bandpass_filter')\n",
      "    bandpass_filter.inputs.highpass_sigma = 0.01\n",
      "    bandpass_filter.inputs.lowpass_sigma = 0.1      \n",
      "    bandpass_filter.inputs.output_datatype = 'short'    \n",
      "    bandpass_filter.inputs.output_type = 'NIFTI'                    \n",
      "    \n",
      "    \"\"\"\n",
      "    Preprocessing-segmentation\n",
      "    \"\"\"\n",
      "    seg = pe.Node(interface = spm.Segment(),name = \"Segmentation\")\n",
      "    seg.inputs.data = anat_file[subject]\n",
      "    seg.inputs.tissue_prob_maps = ['/SCR/datasets-for-tracer-validation/macaque_pri/grey.nii',\n",
      "                                   '/SCR/datasets-for-tracer-validation/macaque_pri/white.nii',\n",
      "                                   '/SCR/datasets-for-tracer-validation/macaque_pri/csf.nii']\n",
      "    seg.inputs.gm_output_type = [False,False,True]  #native space\n",
      "    seg.inputs.csf_output_type = [False,False,True]   #native space\n",
      "    seg.inputs.wm_output_type = [False,False,True]    #native space        \n",
      "    seg.inputs.gaussians_per_class = [2,2,2,4]\n",
      "    seg.inputs.affine_regularization = 'none'\n",
      "    seg.inputs.warping_regularization = 1\n",
      "    seg.inputs.warp_frequency_cutoff = 25\n",
      "    seg.inputs.bias_regularization = 0.0001\n",
      "    seg.inputs.bias_fwhm = 60\n",
      "    seg.inputs.sampling_distance = 3\n",
      "    seg.inputs.save_bias_corrected = True\n",
      "  \n",
      "    \"\"\"\n",
      "    Preprocessing-Normalization\n",
      "    \"\"\"\n",
      "    norm = pe.Node(interface = spm.Normalize(),name = \"Normalization\")\n",
      "    norm.inputs.template = os.path.abspath('/SCR/datasets-for-tracer-validation/tracer-validation/F99/F99_edit_13_cut_T1.nii')\n",
      "    norm.inputs.jobtype = \"estwrite\"\n",
      "    norm.inputs.affine_regularization_type = 'none'\n",
      "    norm.inputs.write_voxel_sizes = [1, 1, 1] \n",
      "    norm.inputs.write_bounding_box = [[-35,-56,-28],[36,38,31]]\n",
      "   \n",
      "    \"\"\"\n",
      "    Extract one volume\n",
      "    \"\"\"    \n",
      "    merge_session = pe.Node(interface =fsl.Merge(), name = 'MergeSession')\n",
      "    merge_session.inputs.dimension = 't'\n",
      "    merge_session.inputs.output_type = 'NIFTI'\n",
      "    \n",
      "    extract_ref = pe.Node(interface=fsl.ExtractROI(t_min=0, t_size=1),name = 'extractref')\n",
      "    \n",
      "    \"\"\"\n",
      "    Preprocessing-Smoothing\n",
      "    \"\"\"\n",
      "    #set up a smoothing node\n",
      "    smoother = pe.Node(interface = spm.Smooth(),name='smoother')\n",
      "    smoother.inputs.fwhm = [3,3,3]\n",
      "\n",
      "    \"\"\"\n",
      "    Functional connectivity\n",
      "    \"\"\"    \n",
      "    ratlas = pe.Node(interface = spmu.ResliceToReference(),name = \"ReslicedAtlas\")\n",
      "    ratlas.inputs.interpolation = 0\n",
      "    ratlas.inputs.in_files = '/SCR/datasets-for-tracer-validation/Project/Markov_Parcellation_3D.nii'\n",
      "    \n",
      "    FConnect = pe.MapNode(util.Function(input_names=['timeseries_file', 'label_file','full_cor'],\n",
      "                                       output_names=['out_ts_file','out_ts_mean_file','out_full_cor_mat','out_full_cor_pval'],\n",
      "                                        function=extract_subrois),iterfield = ['timeseries_file'], name = 'fun_connect')                                                                    \n",
      "            \n",
      "    \"\"\"\n",
      "    Combine sessions and prerequisite normalization\n",
      "    \"\"\"\n",
      "    Session_norm = pe.MapNode(util.Function(input_names=['ts_mean_file'],\n",
      "                                       output_names=['norm_mean_ts'],\n",
      "                                        function=session_norm),iterfield = ['ts_mean_file'], name = 'session_norm')\n",
      "    \n",
      "    Session_cmb = pe.Node(interface = util.Function(input_names=['ts_norm_list','full_cor'],\n",
      "                                        output_names=['cmb_file','out_full_cor_mat','out_full_cor_pval'],function = session_cmb),name = 'session_cmb')\n",
      "                                        \n",
      "    if full_cor:    \n",
      "        FConnect.inputs.full_cor = True\n",
      "        Session_cmb.inputs.full_cor = True\n",
      "    else:\n",
      "        FConnect.inputs.full_cor = False\n",
      "        Session_cmb.inputs.full_cor = False\n",
      "    \n",
      "    corrected_pval = pe.MapNode(util.Function(input_names=['full_pval'],\n",
      "                                       output_names=['out_full_cor_pval'],\n",
      "                                        function=pval_correction),iterfield = ['full_pval'], name = 'adjust_pval')    \n",
      "        \n",
      "    \"\"\"\n",
      "    Result plot\n",
      "    \"\"\"\n",
      "    PlotResult = pe.MapNode(util.Function(input_names=['fun_cor_mat','full_pval'],output_names=['out_image'],function=ResultPlot),iterfield = ['fun_cor_mat','full_pval'],name = 'Result_ploting')\n",
      "    #create and configure a workflow\n",
      "    workflow_name = ''\n",
      "    if glm ==0:   # no glm\n",
      "        if full_cor:\n",
      "            workflow_name = 'preproc_'+subject+'_noglm'+'_fullcor'\n",
      "        else:\n",
      "            workflow_name = 'preproc_'+subject+'_noglm'+'_partial'\n",
      "    \n",
      "    elif glm == 1:  # WM\n",
      "        if full_cor:\n",
      "            workflow_name = 'preproc_'+subject+'_wmglm'+'_fullcor'\n",
      "        else:\n",
      "            workflow_name = 'preproc_'+subject+'_wmglm'+'_partial'\n",
      "    elif glm == 2:  #CSF\n",
      "        if full_cor:\n",
      "            workflow_name = 'preproc_'+subject+'_csfglm'+'_fullcor'\n",
      "        else:\n",
      "            workflow_name = 'preproc_'+subject+'_csfglm'+'_partial'\n",
      "    elif glm == 3:  #WM&CSF\n",
      "        if full_cor:\n",
      "            workflow_name = 'preproc_'+subject+'_wmcsfglm'+'_fullcor'\n",
      "        else:\n",
      "            workflow_name = 'preproc_'+subject+'_wmcsfglm'+'_partial'\n",
      "    else:   # TSNR\n",
      "        if full_cor:\n",
      "            workflow_name = 'preproc_'+subject+'_tsnr'+'_fullcor'\n",
      "        else:\n",
      "            workflow_name = 'preproc_'+subject+'_tsnr'+'_partial'\n",
      "            \n",
      "    \n",
      "    if combined:\n",
      "        workflow_name = workflow_name + '_cmb'\n",
      "\n",
      "        \n",
      "    workflow = pe.Workflow(name=workflow_name)\n",
      "    workflow.base_dir='/SCR/datasets-for-tracer-validation/tracer-validation/'+subject+'/reoriented/'\n",
      "\n",
      "    sinker.inputs.base_directory = op.abspath('/SCR/datasets-for-tracer-validation/tracer-validation/'+subject+'/reoriented/')\n",
      "    \n",
      "    workflow.connect(remove_vol,'roi_file',realigner,'in_files')    \n",
      "    \n",
      "    workflow.connect(realigner, 'realigned_files',sinker, 'realigned')\n",
      "    workflow.connect(realigner, 'realignment_parameters', sinker, 'realigned.@parameters')\n",
      "    \n",
      "    #realign -- coreg\n",
      "    workflow.connect(realigner, \"mean_image\", coreg, \"target\")\n",
      "    #realign--slice timing\n",
      "    workflow.connect(realigner, \"realigned_files\", slicetime, \"in_files\")\n",
      "        \n",
      "    if glm==0: #no glm \n",
      "        \n",
      "        workflow.connect(coreg, \"coregistered_source\", seg, \"data\")  \n",
      "        workflow.connect(coreg,\"coregistered_source\",norm,\"source\")\n",
      "        workflow.connect(seg,\"transformation_mat\",norm,\"parameter_file\")\n",
      "        workflow.connect(slicetime,\"timecorrected_files\",bandpass_filter,\"in_file\")\n",
      "        \n",
      "    elif glm == 1:  # WM\n",
      "        workflow.connect(coreg, \"coregistered_source\", seg, \"data\")  \n",
      "        #seg--norm\n",
      "        workflow.connect(coreg,\"coregistered_source\",norm,\"source\")\n",
      "        workflow.connect(seg,\"transformation_mat\",norm,\"parameter_file\")\n",
      "        #seg-sink        \n",
      "        workflow.connect(seg,'native_wm_image', threshWMseg,'in_file')   \n",
      "        workflow.connect(threshWMseg,'out_file',sinker,'wm_mask')\n",
      "\n",
      "        workflow.connect(realigner,'mean_image',reslicemask,'target')\n",
      "        workflow.connect(threshWMseg,'out_file',reslicemask,'in_files')\n",
      "        workflow.connect(reslicemask,'out_files',sinker,'reslicedmask')\n",
      "        \n",
      "        workflow.connect(slicetime, 'timecorrected_files',compcor, 'realigned_file')\n",
      "        workflow.connect(reslicemask, 'out_files', compcor, 'noise_mask_file')\n",
      "                            \n",
      "        workflow.connect(realigner,'realignment_parameters',compcor,'motion_file')\n",
      "                            \n",
      "        workflow.connect(slicetime, 'timecorrected_files',remove_noise, 'in_file')\n",
      "        workflow.connect(compcor, 'noise_components',remove_noise, 'design_file')      \n",
      "        workflow.connect(remove_noise, 'out_file', bandpass_filter, 'in_file')\n",
      "        \n",
      "    elif glm == 2:  #CSF\n",
      "        workflow.connect(coreg, \"coregistered_source\", seg, \"data\")  \n",
      "        #seg--norm\n",
      "        workflow.connect(coreg,\"coregistered_source\",norm,\"source\")\n",
      "        workflow.connect(seg,\"transformation_mat\",norm,\"parameter_file\")\n",
      "        #seg-sink        \n",
      "        workflow.connect(seg,'native_csf_image', threshCSFseg,'in_file')   \n",
      "        workflow.connect(threshCSFseg,'out_file',sinker,'csf_mask')\n",
      "\n",
      "        workflow.connect(realigner,'mean_image',reslicemask,'target')\n",
      "        workflow.connect(threshCSFseg,'out_file',reslicemask,'in_files')\n",
      "        workflow.connect(reslicemask,'out_files',sinker,'reslicedmask')\n",
      "        \n",
      "        workflow.connect(slicetime, 'timecorrected_files',compcor, 'realigned_file')\n",
      "        workflow.connect(reslicemask, 'out_files', compcor, 'noise_mask_file')\n",
      "                            \n",
      "        workflow.connect(realigner,'realignment_parameters',compcor,'motion_file')\n",
      "                            \n",
      "        workflow.connect(slicetime, 'timecorrected_files',remove_noise, 'in_file')\n",
      "        workflow.connect(compcor, 'noise_components',remove_noise, 'design_file')\n",
      "        \n",
      "        workflow.connect(remove_noise, 'out_file', bandpass_filter, 'in_file')\n",
      "        \n",
      "    elif glm == 3:  #WM&CSF\n",
      "        workflow.connect(coreg, \"coregistered_source\", seg, \"data\")  \n",
      "        #seg--norm\n",
      "        workflow.connect(coreg,\"coregistered_source\",norm,\"source\")\n",
      "        workflow.connect(seg,\"transformation_mat\",norm,\"parameter_file\")\n",
      "        workflow.connect(seg,'native_csf_image', threshCSFseg,'in_file')     \n",
      "        workflow.connect(threshCSFseg,'out_file',sinker,'csf_mask')\n",
      "        \n",
      "        workflow.connect(seg,'native_wm_image', threshWMseg,'in_file')   \n",
      "        workflow.connect(threshWMseg,'out_file',sinker,'wm_mask')\n",
      "        \n",
      "        workflow.connect(threshCSFseg,'out_file',threshbothseg,'in_file')\n",
      "        workflow.connect(threshWMseg,'out_file',threshbothseg,'operand_file')\n",
      "        workflow.connect(threshbothseg,'out_file',sinker,'both_mask')\n",
      "        workflow.connect(realigner,'mean_image',reslicemask,'target')\n",
      "        workflow.connect(threshbothseg,'out_file',reslicemask,'in_files')\n",
      "        workflow.connect(reslicemask,'out_files',sinker,'reslicedmask')\n",
      "        \n",
      "        workflow.connect(slicetime, 'timecorrected_files',compcor, 'realigned_file')\n",
      "        workflow.connect(reslicemask, 'out_files', compcor, 'noise_mask_file')\n",
      "                            \n",
      "        workflow.connect(realigner,'realignment_parameters',compcor,'motion_file')\n",
      "                            \n",
      "        workflow.connect(slicetime, 'timecorrected_files',remove_noise, 'in_file')\n",
      "        workflow.connect(compcor, 'noise_components',remove_noise, 'design_file')\n",
      "        \n",
      "        workflow.connect(remove_noise, 'out_file', bandpass_filter, 'in_file')        \n",
      "    else:       #tsnr      \n",
      "        workflow.connect(slicetime, 'timecorrected_files', tsnr, 'in_file')\n",
      "        workflow.connect(tsnr, 'stddev_file', threshold_stddev, 'in_file')\n",
      "        workflow.connect(tsnr, 'stddev_file', getthresh, 'in_file')\n",
      "        workflow.connect(getthresh, 'out_stat', threshold_stddev, 'thresh')\n",
      "        workflow.connect(threshold_stddev,'out_file',sinker,'noise_mask')\n",
      "        workflow.connect(slicetime, 'timecorrected_files',compcor, 'realigned_file')\n",
      "        workflow.connect(threshold_stddev, 'out_file', compcor, 'noise_mask_file')\n",
      "                            \n",
      "        workflow.connect(realigner,'realignment_parameters',compcor,'motion_file')\n",
      "                            \n",
      "        workflow.connect(tsnr, 'detrended_file',remove_noise, 'in_file')\n",
      "        workflow.connect(compcor, 'noise_components',remove_noise, 'design_file')\n",
      "        \n",
      "        workflow.connect(remove_noise, 'out_file', bandpass_filter, 'in_file')\n",
      "         \n",
      "        workflow.connect(coreg, \"coregistered_source\", seg, \"data\")       \n",
      "        #seg--norm\n",
      "        workflow.connect(coreg,\"coregistered_source\",norm,\"source\")\n",
      "        workflow.connect(seg,\"transformation_mat\",norm,\"parameter_file\")\n",
      "    \n",
      "   \n",
      "    workflow.connect(seg,'bias_corrected_image',sinker,'seg_bias_corrected')\n",
      "    workflow.connect(seg,'inverse_transformation_mat',sinker,'seg_inverse_trans')\n",
      "    workflow.connect(seg,'native_gm_image',sinker,'seg_mask_gm')\n",
      "    workflow.connect(seg,'native_wm_image',sinker,'seg_mask_wm')\n",
      "    workflow.connect(seg,'native_csf_image',sinker,'seg_mask_csf')\n",
      " \n",
      "    workflow.connect(bandpass_filter,\"out_file\",norm,\"apply_to_files\")    \n",
      "    \n",
      "    workflow.connect(norm,\"normalized_files\",sinker,\"norm\")       \n",
      "    workflow.connect(norm,\"normalized_files\",smoother,\"in_files\")   \n",
      "    \n",
      "    workflow.connect(norm,'normalized_files',merge_session,'in_files')\n",
      "    workflow.connect(merge_session,'merged_file',sinker,'merged_file')\n",
      "    workflow.connect(merge_session,'merged_file',extract_ref,'in_file')\n",
      "    \n",
      "    workflow.connect(extract_ref,'roi_file',sinker,'refvol')\n",
      "    workflow.connect(extract_ref,'roi_file',ratlas,'target')\n",
      "    workflow.connect(ratlas,'out_files',sinker,'RAtlas')     \n",
      "    \n",
      "    if combined:          \n",
      "        workflow.connect(ratlas,'out_files',FConnect,\"label_file\")\n",
      "        workflow.connect(smoother,\"smoothed_files\",FConnect,\"timeseries_file\")\n",
      "        \n",
      "        workflow.connect(FConnect,\"out_ts_file\",sinker,\"whole_timeseries\")\n",
      "        workflow.connect(FConnect,\"out_ts_mean_file\",sinker,\"mean_timeseries\")\n",
      "        workflow.connect(FConnect,\"out_full_cor_mat\",sinker,\"full_cor_mat\")\n",
      "        \n",
      "        workflow.connect(FConnect,'out_ts_mean_file',Session_norm,'ts_mean_file')    \n",
      "        workflow.connect(Session_norm,'norm_mean_ts',sinker,'norm_mean_ts')\n",
      "        workflow.connect(Session_norm,'norm_mean_ts',Session_cmb,'ts_norm_list')\n",
      "        \n",
      "        workflow.connect(Session_cmb,\"out_full_cor_mat\",PlotResult,\"fun_cor_mat\")\n",
      "        workflow.connect(Session_cmb,'out_full_cor_pval',corrected_pval,'full_pval')\n",
      "        workflow.connect(corrected_pval,'out_full_cor_pval',sinker,'corrected_pval')\n",
      "        workflow.connect(corrected_pval,'out_full_cor_pval',PlotResult,'full_pval')\n",
      "        workflow.connect(PlotResult,\"out_image\",sinker,\"plot\") \n",
      "    else:\n",
      "        workflow.connect(ratlas,'out_files',FConnect,\"label_file\")\n",
      "        workflow.connect(smoother,\"smoothed_files\",FConnect,\"timeseries_file\")\n",
      "        \n",
      "        workflow.connect(FConnect,\"out_ts_file\",sinker,\"whole_timeseries\")\n",
      "        workflow.connect(FConnect,\"out_ts_mean_file\",sinker,\"mean_timeseries\")\n",
      "        workflow.connect(FConnect,\"out_full_cor_mat\",sinker,\"full_cor_mat\")\n",
      "        \n",
      "        workflow.connect(FConnect,\"out_full_cor_mat\",PlotResult,\"fun_cor_mat\")        \n",
      "        workflow.connect(FConnect,'out_full_cor_pval',corrected_pval,'full_pval')\n",
      "        workflow.connect(corrected_pval,'out_full_cor_pval',sinker,'corrected_pval')\n",
      "        workflow.connect(corrected_pval,'out_full_cor_pval',PlotResult,'full_pval')\n",
      "        workflow.connect(PlotResult,\"out_image\",sinker,\"plot\") \n",
      "    #visualize the workflow\n",
      "    workflow.write_graph()               \n",
      "    workflow.run()  \n",
      "\n",
      "# subject name, glm/not,full/partial\n",
      "\"\"\"\n",
      "parameters:\n",
      "1. Subject id\n",
      "2. GLM options {0,1,2,3,4}\n",
      "    0: no glm\n",
      "    1: noise ROI from white matter(WM) only\n",
      "    2: noise ROI from CSF alone\n",
      "    3: noise ROI from WM&CSF \n",
      "    4: noise ROI from functional image with highest viability of TSNR\n",
      "3. Correlation\n",
      "    True: full correlation\n",
      "    False: partial correlation\n",
      "    \n",
      "4. Session-combined correlation(True)/Session-separate correlation(False)  \n",
      "\"\"\"\n",
      "\n",
      "glm = [1,2,3]\n",
      "cor = [True,False]\n",
      "session = [True,False]\n",
      "for loop1 in glm:\n",
      "    for loop2 in cor:\n",
      "        for loop3 in session:         \n",
      "            Benchmark_Calculation(\"M5\",loop1,loop2,loop3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}